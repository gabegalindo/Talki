# services.py
# -*- coding: utf-8 -*-
import os
import vertexai
from vertexai.preview.vision_models import ImageGenerationModel
from vertexai.generative_models import GenerativeModel
from pydub import AudioSegment
from google.cloud import speech, texttospeech
import uuid

# --- Vertex AI ëª¨ë¸ ì „ì—­ ë³€ìˆ˜ ---
# app.pyì—ì„œ ì´ˆê¸°í™”ëœ í›„ ì‚¬ìš©ë  ëª¨ë¸ë“¤ì„ ë‹´ì„ ë³€ìˆ˜ì…ë‹ˆë‹¤.
image_generation_model = None
generative_model = None

# --- 0. Vertex AI ëª¨ë¸ ì´ˆê¸°í™” í•¨ìˆ˜ ---
def initialize_vertex_ai(project_id, location):
    """
    Vertex AIë¥¼ ì´ˆê¸°í™”í•˜ê³  ì‚¬ìš©í•  ëª¨ë¸(ì´ë¯¸ì§€ ìƒì„±, Gemini)ë“¤ì„
    ì „ì—­ ë³€ìˆ˜ì— ë¡œë“œí•©ë‹ˆë‹¤. app.pyì—ì„œ ì„œë²„ ì‹œì‘ ì‹œ í•œë²ˆë§Œ í˜¸ì¶œë©ë‹ˆë‹¤.
    """
    global image_generation_model, generative_model
    try:
        vertexai.init(project=project_id, location=location)
        image_generation_model = ImageGenerationModel.from_pretrained("imagegeneration@006")
        # ì‚¬ìš© ê°€ëŠ¥í•œ Gemini ëª¨ë¸ ë²„ì „ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        generative_model = GenerativeModel("gemini-1.5-flash")  # ì•ˆì •ì ì¸ ìµœì‹  ë²„ì „ ì‚¬ìš©
        print(f"âœ… Vertex AIê°€ í”„ë¡œì íŠ¸ '{project_id}', ë¦¬ì „ '{location}'ìœ¼ë¡œ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True
    except Exception as e:
        print(f"ğŸš¨ Vertex AI ì´ˆê¸°í™” ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

# --- 1. ì´ë¯¸ì§€ ìƒì„± ì„œë¹„ìŠ¤ ---
def generate_image_service(prompt, output_path):
    """ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê³  ì§€ì •ëœ ê²½ë¡œì— ì €ì¥í•©ë‹ˆë‹¤."""
    if not image_generation_model:
        print("ğŸš¨ ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
    try:
        images = image_generation_model.generate_images(
            prompt=prompt,
            number_of_images=1
        )
        images[0].save(location=output_path, include_generation_parameters=True)
        return True
    except Exception as e:
        print(f"ğŸš¨ ì´ë¯¸ì§€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

def convert_webm_to_wav(webm_path, wav_path):
    """WebM íŒŒì¼ì„ GCP STTê°€ ì¸ì‹ ê°€ëŠ¥í•œ WAV íŒŒì¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    try:
        audio = AudioSegment.from_file(webm_path, format="webm")
        # GCP STT ê¶Œì¥ ì‚¬ì–‘: ë‹¨ì¼ ì±„ë„, 16000Hz, 16ë¹„íŠ¸
        audio = audio.set_channels(1)
        audio = audio.set_frame_rate(16000)
        audio = audio.set_sample_width(2) # [ì¶”ê°€] ìƒ˜í”Œë§ ë¹„íŠ¸ë¥¼ 16ë¹„íŠ¸ë¡œ ì„¤ì • (2ë°”ì´íŠ¸)
        
        audio.export(wav_path, format="wav")
        print(f"âœ… ì˜¤ë””ì˜¤ ë³€í™˜ ì„±ê³µ: {webm_path} -> {wav_path}")
        return True
    except Exception as e:
        print(f"ğŸš¨ ì˜¤ë””ì˜¤ ë³€í™˜ ì‹¤íŒ¨: {e}")
        return False

# --- 3. ìŒì„± í…ìŠ¤íŠ¸ ë³€í™˜(STT) ì„œë¹„ìŠ¤ ---
def transcribe_audio(audio_path):
    """WAV ì˜¤ë””ì˜¤ íŒŒì¼ì˜ ë‚´ìš©ì„ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        audio_path (str): ë³€í™˜í•  ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        
    Returns:
        str: ì¸ì‹ëœ í…ìŠ¤íŠ¸ ë˜ëŠ” ì˜¤ë¥˜ ì‹œ None
    """
    try:
        # ì˜¤ë””ì˜¤ íŒŒì¼ì„ 16ë¹„íŠ¸ PCM í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        audio = AudioSegment.from_file(audio_path)
        audio = audio.set_sample_width(2)  # 16ë¹„íŠ¸ë¡œ ì„¤ì • (2ë°”ì´íŠ¸ = 16ë¹„íŠ¸)
        audio = audio.set_channels(1)  # ëª¨ë…¸ ì±„ë„ë¡œ ì„¤ì •
        audio = audio.set_frame_rate(16000)  # 16kHz ìƒ˜í”Œ ë ˆì´íŠ¸ë¡œ ì„¤ì •
        
        # static/generated_audio ë””ë ‰í† ë¦¬ ìƒì„± (ì—†ëŠ” ê²½ìš°)
        output_dir = os.path.join('static', 'generated_audio')
        os.makedirs(output_dir, exist_ok=True)
        
        # ê³ ìœ í•œ íŒŒì¼ëª… ìƒì„±
        temp_filename = f'temp_{uuid.uuid4()}.wav'
        temp_path = os.path.join(output_dir, temp_filename)
        
        # ë³€í™˜ëœ ì˜¤ë””ì˜¤ íŒŒì¼ ì €ì¥
        audio.export(temp_path, format='wav')
        print(f"âœ… ì„ì‹œ ì˜¤ë””ì˜¤ íŒŒì¼ ì €ì¥: {temp_path}")
            
        # ë³€í™˜ëœ ì˜¤ë””ì˜¤ íŒŒì¼ ì½ê¸°
        with open(temp_path, 'rb') as audio_file:
            content = audio_file.read()
            
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        try:
            os.unlink(temp_path)
            print(f"âœ… ì„ì‹œ íŒŒì¼ ì‚­ì œ: {temp_path}")
        except Exception as e:
            print(f"âš ï¸ ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ ({temp_path}): {e}")
            
        # Google STT API í˜¸ì¶œ
        client = speech.SpeechClient()
        audio = speech.RecognitionAudio(content=content)
        config = speech.RecognitionConfig(
            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
            sample_rate_hertz=16000,
            language_code="en-US",
        )
        response = client.recognize(config=config, audio=audio)
        
        if not response.results:
            return ""
            
        return response.results[0].alternatives[0].transcript
    except Exception as e:
        print(f"ğŸš¨ ìŒì„± ì¸ì‹(STT) ì‹¤íŒ¨: {e}")
        return None

# --- 4. í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ ì„œë¹„ìŠ¤ ---
def analyze_emotion_with_gemini(text):
    """Geminië¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ 'ê¸°ì¨', 'ìŠ¬í””', 'ì¤‘ë¦½' ì¤‘ í•˜ë‚˜ë¡œ ì•ˆì •ì ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not generative_model:
        print("ğŸš¨ ê°ì • ë¶„ì„ ëª¨ë¸(Gemini)ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return "ì¤‘ë¦½"
    if not text:
        return "ì¤‘ë¦½"
        
    try:
        prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ëŠê»´ì§€ëŠ” ê°€ì¥ í•µì‹¬ì ì¸ ê°ì •ì„ 'ê¸°ì¨', 'ìŠ¬í””', 'ì¤‘ë¦½' ì¤‘ í•˜ë‚˜ë¡œë§Œ ë¶„ë¥˜í•´ì¤˜. ë‹¤ë¥¸ ì„¤ëª…ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆ.

í…ìŠ¤íŠ¸: "{text}"
"""
        response = generative_model.generate_content(prompt)
        response_text = response.text.strip()
        
        if 'ê¸°ì¨' in response_text:
            return 'ê¸°ì¨'
        elif 'ìŠ¬í””' in response_text:
            return 'ìŠ¬í””'
        else:
            return 'ì¤‘ë¦½'
    except Exception as e:
        print(f"ğŸš¨ ê°ì • ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        return "ì¤‘ë¦½"

# --- 5. Gemini ëŒ€í™” ì‘ë‹µ ìƒì„± ì„œë¹„ìŠ¤ ---
def generate_conversational_response(user_text: str) -> str:
    """ì‚¬ìš©ìì˜ ì…ë ¥ í…ìŠ¤íŠ¸ì— ëŒ€í•œ AIì˜ ëŒ€í™”í˜• ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    if not generative_model:
        print("ğŸš¨ ì‘ë‹µ ìƒì„± ëª¨ë¸(Gemini)ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return "Sorry, I can't answer that."
    if not user_text:
        return "Sorry, I can't answer that."
        
    try:
        prompt = f"""
        You are a friendly AI assistant. 
        Please respond to the user's message in a warm and concise manner (1-2 sentences). 
        Always respond in English.
        
        [prompt]
        Use short, simple sentences with one clear idea each. 
        Avoid difficult or abstract words. 
        Give lots of positive feedback and praise. 
        Reduce negative words. 
        Use a wait-friendly tone like â€œTake your timeâ€. 
        Provide emotional support like â€œI understandâ€ or â€œThat sounds funâ€. 
        Repeat important points when needed.
        Give meaningful suggestions based on the emotion of user input.
        
        User: "{user_text}"
        AI:"""
        
        response = generative_model.generate_content(prompt)
        return response.text.strip() if response.text else "Hmm... I'm not sure how to respond to that."
        
    except Exception as e:
        print(f"ğŸš¨ Gemini ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        return "Sorry, I can't answer that."

# --- 6. í…ìŠ¤íŠ¸ ìŒì„± ë³€í™˜(TTS) ì„œë¹„ìŠ¤ ---
def text_to_speech_service(text_to_speak, output_filename, emotion="NEUTRAL"):
    """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ê°ì •ì´ ë‹´ê¸´ í•œêµ­ì–´ ìŒì„± íŒŒì¼(MP3)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    ssml_template = {
        "ê¸°ì¨": f'<speak><prosody rate="fast" pitch="+2st">{text_to_speak}</prosody></speak>',
        "ìŠ¬í””": f'<speak><prosody rate="slow" pitch="-2st">{text_to_speak}</prosody></speak>',
        "ì¤‘ë¦½": f'<speak>{text_to_speak}</speak>'
    }
    ssml_text = ssml_template.get(emotion.upper(), ssml_template["ì¤‘ë¦½"])
    
    try:
        client = texttospeech.TextToSpeechClient()
        synthesis_input = texttospeech.SynthesisInput(ssml=ssml_text)
        
        voice = texttospeech.VoiceSelectionParams(
            language_code="en-US", 
            name="en-US-Wavenet-A"
        )
        
        audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)

        response = client.synthesize_speech(
            input=synthesis_input, voice=voice, audio_config=audio_config
        )

        with open(output_filename, "wb") as out:
            out.write(response.audio_content)
        
        return True
    
    except Exception as e:
        print(f"ğŸš¨ TTS ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False